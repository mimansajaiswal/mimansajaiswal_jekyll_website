<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Mimansa Jaiswal</title>

        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

        <meta name="author" content="Abhishek Das">
        <meta name="keywords" content="yoda, abhshkdz, abhishek das, sdslabs, jekyll, hacker, developer, pianist, iit roorkee, abdasuee, deep learning, computer vision, neural networks, georgia tech, virginia tech, machine learning">
        
        <meta name="description" content="CS PhD student at Georgia Tech. Learning to build machines that can see, think, talk and act.">
        
        <link rel="alternate" type="application/atom+xml" title="Abhishek Das" href="https://abhishekdas.com/atom.xml" />
        <!--Open Graph Related Stuff-->
        <meta property="fb:app_id" content="1666933880208467"/>
        <meta property="og:title" content=""/>
        <meta property="og:url" content="http://abhishekdas.com/"/>
        
        <meta name='og:description' content='CS PhD student at Georgia Tech. Learning to build machines that can see, think, talk and act.'/>
        
        
        <meta property="og:site_name" content="Abhishek Das"/>
        
        <meta property="og:image" content="http://abhishekdas.com/img/avatar.jpg"/>
        <meta property="og:image:url" content="http://abhishekdas.com/img/avatar.jpg"/>
        

        <!--Twitter Card Stuff-->
        <meta name='twitter:card' content='summary_large_image'/>
        
        <meta name='twitter:title' content='Abhishek Das'/>
        
        
        <meta name='twitter:image' content='http://abhishekdas.com/img/avatar.jpg'/>
        
        <meta name='twitter:creator' content='@abhshkdz'/>
        <meta name='twitter:site' content='@abhshkdz'/>
        <meta name='twitter:url' content='http://abhishekdas.com/'/>
        
        <meta name='twitter:description' content='CS PhD student at Georgia Tech. Learning to build machines that can see, think, talk and act.'/>
        

        <link rel="stylesheet" href="/css/bootstrap.min.css">
        <link rel="stylesheet" href="/css/main_1548935627.css">
        <!--[if IE 7]>
            <html class="ie7">
            <link rel="stylesheet" type="text/css" href="/css/font-awesome-ie7.min.css">
        <![endif]-->
        <!--[if IE 8]><html class="ie8"> <![endif]-->
        <!--[if lt IE 9]>
          <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->

        <link rel="apple-touch-icon" sizes="57x57" href="/ico/apple-icon-57x57.png">
        <link rel="apple-touch-icon" sizes="60x60" href="/ico/apple-icon-60x60.png">
        <link rel="apple-touch-icon" sizes="72x72" href="/ico/apple-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="76x76" href="/ico/apple-icon-76x76.png">
        <link rel="apple-touch-icon" sizes="114x114" href="/ico/apple-icon-114x114.png">
        <link rel="apple-touch-icon" sizes="120x120" href="/ico/apple-icon-120x120.png">
        <link rel="apple-touch-icon" sizes="144x144" href="/ico/apple-icon-144x144.png">
        <link rel="apple-touch-icon" sizes="152x152" href="/ico/apple-icon-152x152.png">
        <link rel="apple-touch-icon" sizes="180x180" href="/ico/apple-icon-180x180.png">
        <link rel="icon" type="image/png" sizes="192x192"  href="/ico/android-icon-192x192.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/ico/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="96x96" href="/ico/favicon-96x96.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/ico/favicon-16x16.png">
        <link rel="shortcut icon" href="/ico/favicon-96x96.png">
        <link rel="manifest" href="/ico/manifest.json">
        <meta name="msapplication-TileColor" content="#ffffff">
        <meta name="msapplication-TileImage" content="/ico/ms-icon-144x144.png">
        <meta name="theme-color" content="#ffffff">

        <!--Authorship-->
        <link rel="me" href="//plus.google.com/u/0/100610196494221761914/">
        <link rel="me" href="//facebook.com/abhshkdz">

        <!--[if lt IE 9]>
            <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->

    </head>
    <body>
        <!-- Header
            ================================================== -->
        <div class="top-strip"></div>
        <div class="container">
            <div class="page-header">
                <div class="row">
                    <div class="col-xs-12">
                        <h1 id="name-block"><a href="/">Abhishek Das</b></a></h1>
                    </div>
                    <div class="col-xs-12">
                        <ul class="nav-bar">
                            <li><a href="/#/bio">Bio</a></li>
                            <li><a target="_blank" href="https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing">CV</a></li>
                            <!-- <li><a href="/archive">Blog</a></li> -->
                            <li><a href="/#/publications">Publications</a></li>
                            <li class="hide-link-on-small-screens"><a href="/#/talks">Talks</a></li>
                            <li class="hide-link-on-small-screens"><a href="/bookshelf">Bookshelf</a></li>
                            <!-- <li class="hide-link-on-small-screens"><a href="/#/projects">Side projects</a></li> -->
                            <li><a target="_blank" href="//github.com/abhshkdz">GitHub</a></li>
                            <li><a target="_blank" href="https://scholar.google.com/citations?user=t6exkOAAAAAJ">Google Scholar</a></li>
                            <li><a target="_blank" href="//twitter.com/abhshkdz">Twitter</a></li>
                            <!-- <li><a target="_blank" href="//x.abhishekdas.com">Tumblr</a></li> -->
                            <li class="hide-link-on-small-screens"><a target="_blank" href="//mastodon.social/web/accounts/1011404">Mastodon</a></li>
                            <li class="hide-link-on-small-screens"><a target="_blank" href="//instagram.com/abhshkdz">Instagram</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="page-content">
                <article class="post">
    <div class="row">
        <div class="col-xs-12">
            <h1 class="content"></h1>
        </div>
    </div>
    <section class="post-content">
        <div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg" />
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Research Scientist<br />
        Facebook AI Research<br />
        abhshkdz at fb dot com
    </div>
</div>
<hr />

<p><a name="/news"></a></p>

<h1 id="news">News</h1>

<ul>
  <li>[Nov 2019] Organizing the <a href="http://visualqa.org/workshop.html">Visual Question Answering and Dialog workshop at CVPR 2020</a>.</li>
  <li>[Sep 2019] Organizing the <a href="https://vigilworkshop.github.io">Visually-Grounded Interaction &amp; Language Workshop at NeurIPS</a>.</li>
  <li>[Jun 2019] Presenting <a href="#/multi-agent-comm">Targeted Multi-Agent Communication</a> as an oral at ICML 2019 (<a href="https://www.facebook.com/icml.imls/videos/444326646299556/">Video</a>).</li>
  <li>[Mar 2019] Co-founded <a href="https://caliper.ai">Caliper</a>. Caliper helps recruiters evaluate practical AI skills.</li>
  <li>[Feb 2019] My work was featured in this <a href="https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents">wonderful article by Georgia Tech</a>.</li>
  <li>[Jan 2019] Awarded the <a href="https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/">Facebook Graduate Fellowship</a>.</li>
  <li>[Jan 2019] Awarded the Microsoft Research PhD Fellowship (declined).</li>
  <li>[Jan 2019] Awarded the NVIDIA Graduate Fellowship (declined).</li>
  <li>[Jan 2019] Organizing the <a href="https://visualdialog.org/challenge/2019">2nd Visual Dialog Challenge</a>!</li>
  <li>[Oct 2018] Presenting <a href="#/eqa-modular">Neural Modular Control for Embodied Question Answering</a> as a spotlight at CoRL 2018 (<a href="https://www.youtube.com/watch?v=xoHvho-YRgs&amp;t=7330">Video</a>).</li>
  <li>[Sep 2018] Presenting <a href="https://visualdialog.org/challenge/2018#winners">results and analysis of the 1st Visual Dialog Challenge</a> at ECCV 2018.</li>
  <li>[Jul 2018] Presenting a <a href="https://lvatutorial.github.io/">tutorial on Connecting Language and Vision to Actions</a> at <a href="http://acl2018.org/tutorials/#connecting-language-and-vis">ACL 2018</a>.</li>
  <li>[Jun 2018] Organizing the 1st <a href="https://visualdialog.org/challenge/2018">Visual Dialog Challenge</a>!</li>
  <li>[Jun 2018] Presenting <a href="#/embodied-qa">Embodied Question Answering</a> as an oral at CVPR 2018 (<a href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s">Video</a>).</li>
  <li>[Jun 2018] Organizing the <a href="http://visualqa.org/workshop.html">VQA Challenge and Visual Dialog Workshop at CVPR 2018</a>.</li>
  <li>[Mar 2018] Speaking on <a href="https://embodiedqa.org/">Embodied Question Answering</a> at <a href="https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715">NVIDIA GTC</a> (<a href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">Video</a>).</li>
  <li>[Dec 2017] Awarded the <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/">Adobe Research Fellowship</a>. (<a href="https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud">Department’s news story</a>)</li>
  <li>[Dec 2017] Awarded the <a href="https://snapresearchfellowship.splashthat.com/">Snap Inc. Research Fellowship</a>. (<a href="https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards">Department’s news story</a>)</li>
  <li>[Oct 2017] Presenting our paper on <a href="#/visdial-rl">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</a> as an oral at ICCV 2017 (<a href="https://www.youtube.com/watch?v=R4hugGnNr7s">Video</a>).</li>
  <li>[Jul 2017] Speaking about our work on <a href="//visualdialog.org">Visual Dialog</a> at the <a href="http://visualqa.org/workshop.html">Visual Question Answering Challenge Workshop</a>, CVPR 2017 (<a href="https://youtu.be/KAlGWMJnWyc?t=26m56s">Video</a>).</li>
  <li>[Jul 2017] Presenting our paper on <a href="#/visdial">Visual Dialog</a> as a spotlight at CVPR 2017 (<a href="https://www.youtube.com/watch?v=I9OlorMh7wU">Video</a>).</li>
</ul>

<div id="read-more-button">
    <a nohref="">Read more</a>
</div>

<hr />

<p><a name="/bio"></a></p>

<h1 id="bio">Bio</h1>

<p>I am a Research Scientist at Facebook AI Research (FAIR).
Previously, I was a Computer Science PhD student at Georgia Tech, advised by <a href="///www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
and working closely with <a href="//www.cc.gatech.edu/~parikh/">Devi Parikh</a>.
My PhD research was supported by fellowships from <a href="https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/">Facebook</a>, <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/">Adobe</a>, and <a href="https://snapresearchfellowship.splashthat.com/">Snap</a>.</p>

<p>My research focuses on deep learning
and its applications in climate change, and in building agents that can <i>see</i> (computer vision),
<i>think</i> (reasoning/interpretability), <i>talk</i> (language modeling), and
<i>act</i> (reinforcement learning).
My CV is available <a href="https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing">here</a>.</p>

<div class="row" id="timeline-logos">
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//iitr.ac.in"><img src="/img/logos/iitr.jpg" /></a>
        </div>
        <div class="logo-desc">
            IIT Roorkee<br />
            2011 - 2015
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//qbi.uq.edu.au"><img style="width:120px;" src="/img/logos/uq.png" /></a>
        </div>
        <div class="logo-desc">
            Queensland Brain Institute<br />
            Summer 2015
        </div>
    </div>
    <!-- <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//vt.edu"><img src="/img/logos/vt.png"></a>
        </div>
        <div class="logo-desc">
            Virginia Tech<br>
            2015 - 2016
        </div>
    </div> -->
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//gatech.edu"><img src="/img/logos/gatech.png" /></a>
        </div>
        <div class="logo-desc">
            Georgia Tech<br />
            2017 - Present
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png" /></a>
        </div>
        <div class="logo-desc">
            Facebook AI Research<br />
            S2017, W2018, S2018
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//deepmind.com"><img style="width:120px;" src="/img/logos/deepmind.png" /></a>
        </div>
        <div class="logo-desc">
            DeepMind<br />
            W2019
        </div>
    </div>
</div>

<p>I’ve spent three wonderful semesters as an intern at Facebook AI Research — Summer 2017 and Spring 2018 at Menlo Park, working with <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>, <a href="https://research.fb.com/people/parikh-devi/">Devi Parikh</a> and <a href="https://research.fb.com/people/batra-dhruv/">Dhruv Batra</a> on training embodied agents for navigation and question-answering in simulated environments (see <a href="https://embodiedqa.org/">embodiedqa.org</a>), and Summer 2018 at Montréal, working with <a href="https://research.fb.com/people/rabbat-mike/">Mike Rabbat</a> and <a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a> on emergent communication protocols in large-scale multi-agent reinforcement learning.</p>

<p>In 2019, I was fortunate to get the opportunity to spend time at DeepMind in London working on grounded language learning with <a href="felix-hill">Felix Hill</a>, <a href="laura-rimell">Laura Rimell</a>, and <a href="stephen-clark">Stephen Clark</a>, and at Tesla Autopilot in Palo Alto working on differentiable neural architecture search with <a href="andrej-karpathy">Andrej Karpathy</a>.</p>

<p>Prior to joining grad school, I worked on neural coding in zebrafish tectum
as an intern under <a href="//www.qbi.uq.edu.au/professor-geoffrey-goodhill">Prof. Geoffrey Goodhill</a> and <a href="//researchers.uq.edu.au/researcher/2490">Lilach Avitan</a>
at the <a href="http://cns.qbi.uq.edu.au/">Goodhill Lab</a>, Queensland Brain Institute.</p>

<p>I graduated from <a href="http://iitr.ac.in/">Indian Institute of Technology Roorkee</a> in 2015.
During my undergrad years, I’ve been selected twice for
Google Summer of Code (<a href="/posts/summer-of-code/">2013</a> and <a href="/posts/gsoc-reunion-2014/">2014</a>),
won several hackathons and security contests (<a href="//blog.sdslabs.co/2012/09/hacku">Yahoo! HackU!</a>,
<a href="//blog.sdslabs.co/2014/02/code-fun-do">Microsoft Code.Fun.Do.</a>, Deloitte CCTC <a href="//www.facebook.com/SDSLabs/posts/527540147292475">2013</a> and <a href="/posts/deloitte-cctc-3/">2014</a>),
and been an active member of <a href="//sdslabs.co/">SDSLabs</a>.</p>

<p>On the side, I built <a href="//github.com/abhshkdz/neural-vqa">neural-vqa</a>, an efficient Torch implementation for visual question answering (and its extension <a href="//github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a>),
and maintain <a href="http://aideadlin.es/">aideadlin.es</a> (countdowns to a bunch of CV/NLP/ML/AI conference deadlines),
and several other side projects (<a href="//github.com/abhshkdz/HackFlowy">HackFlowy</a>, <a href="//github.com/abhshkdz/graf">graf</a>, <a href="//github.com/abhshkdz">etc</a>).
I also help maintain <a href="//erdos.sdslabs.co/">Erdős</a>, a competitive math learning platform I created during my undergrad.
I often <a href="//twitter.com/abhshkdz">tweet</a>, <a href="https://mastodon.social/web/accounts/1011404">toot</a>, and post pictures from my <a href="https://conquer.earth/abhshkdz">travels</a> on <a href="//instagram.com/abhshkdz">Instagram</a> and <a href="http://x.abhishekdas.com/">Tumblr</a>.</p>

<p><a href="/archive">Blog posts from a previous life.</a></p>

<hr />

<p><a name="/publications"></a></p>

<h1 id="publications">Publications</h1>

<p><a name="/visdial-bert"></a></p>
<h2 class="pubt">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</h2>
<p class="pubd">
    <span class="authors">Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1912.02379">Paper</a>
        <a target="_blank" href="https://github.com/vmurahari3/visdial-bert">Code</a>
    </span>
</p>
<p><img src="/img/visdial/visdial-bert.png" /></p>
<hr />

<p><a name="/ds-vic"></a></p>
<h2 class="pubt">IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL</h2>
<p class="pubd">
    <span class="authors">Nirbhay Modhe, Prithvijit Chattopadhyay, Mohit Sharma, Abhishek Das, Devi Parikh, Dhruv Batra, Ramakrishna Vedantam</span><br />
    <span class="conf">IJCAI-PRICAI 2020, ICLR 2019 Task-Agnostic RL Workshop</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1907.10580">Paper</a>
    </span>
</p>
<p><img src="/img/ds-vic/teaser.jpg" /></p>
<hr />

<p><a name="/visdial-rl-plus-plus"></a></p>
<h2 class="pubt">Improving Generative Visual Dialog by Answering Diverse Questions</h2>
<p class="pubd">
    <span class="authors">Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh, Abhishek Das</span><br />
    <span class="conf">EMNLP 2019</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.10470">Paper</a>
        <a target="_blank" href="https://github.com/vmurahari3/visdial-diversity">Code</a>
    </span>
</p>
<p><img src="/img/visdial/visdial-rl-plus-plus.png" /></p>
<hr />

<p><a name="/multi-agent-comm"></a></p>
<h2 class="pubt">TarMAC: Targeted Multi-Agent Communication</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael Rabbat, Joelle Pineau</span><br />
    <span class="conf">ICML 2019</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1810.11187">Paper</a>
        <a target="_blank" href="https://drive.google.com/open?id=1ZjKogiYrqFVuBmad3IzkNW18pvP2QcIG">Slides</a>
    </span>
</p>
<p><img src="/img/multi-agent-comm/model.jpg" />
<br /><br />
<img src="/img/multi-agent-comm/shapes.gif" /></p>
<hr />

<p><a name="/eqa-mp3d"></a></p>
<h2 class="pubt">Embodied Question Answering in Photorealistic Environments with Point Clouds</h2>
<p class="pubd">
    <span class="authors">
        Erik Wijmans*, Samyak Datta*, Oleksandr Maksymets*, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra
    </span><br />
    <span class="conf">CVPR 2019 (Oral)</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1904.03461">Paper</a>
    </span>
</p>
<p><img src="/img/eqa/eqa-mp3d.png" /></p>
<hr />

<p><a name="/avsd"></a></p>
<h2 class="pubt">Audio-Visual Scene-Aware Dialog</h2>
<p class="pubd">
    <span class="authors">
        Huda Alamri, Vincent Cartillier, Abhishek Das,
        Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh,
        Dhruv Batra, Anoop Cherian, Tim K. Marks, Chiori Hori
    </span><br />
    <span class="conf">CVPR 2019</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1901.09107">Paper</a>
        <a target="_blank" href="https://github.com/batra-mlp-lab/avsd">Code</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<p><img src="/img/avsd/avsd.jpg" /></p>
<hr />

<p><a name="/avsd_icassp"></a></p>
<h2 class="pubt">End-to-end Audio Visual Scene-Aware Dialog Using Multimodal Attention-based Video Features</h2>
<p class="pubd">
    <span class="authors">
            Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh
    </span><br />
    <span class="conf">ICASSP 2019</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1806.08409">Paper</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<p><img src="/img/avsd/avsd_icassp.jpg" /></p>
<hr />

<p><a name="/eqa-modular"></a></p>
<h2 class="pubt">Neural Modular Control for Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br />
    <span class="conf">CoRL 2018 (Spotlight)</span><br />
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1810.11181">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&amp;t=7330">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=1xTvgpVNxG7MPZQe6jtXuYUIT2WPtoh0U">Slides</a>
    </span>
</p>

<p><img src="/img/eqa/eqa-modular.png" /></p>

<hr />

<p><a name="/embodied-qa"></a></p>
<h2 class="pubt">Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br />
    <span class="conf">CVPR 2018 (Oral)</span><br />
    <span class="links">
        <a target="_blank" href="https://embodiedqa.org/paper.pdf">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://github.com/facebookresearch/EmbodiedQA">Code</a>
        <a target="_blank" href="//youtu.be/gz2VoDrvX-A?t=1h19m58s">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=1UacybW4p_8PDPNUvnEl05_89tbeG0ItP">Slides</a>
    </span>
    <!-- Press: -->
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://mlatgt.blog/2018/02/26/embodied-question-answering/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/mlgt.png" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Embodied Question Answering" by Abhishek Das</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://code.facebook.com/posts/1622140391226436/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... a goal-driven approach to autonomous agents" by Dhruv Batra, Devi Parikh</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/611040/facebook-helped-create-an-ai-scavenger-hunt-that-could-lead-to-the-first-useful-home-robots/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... an AI scavenger hunt that could lead to the first useful home robots" by Will Knight</span>
            </div>
        </a>
    </div>
</p>

<p><img src="/img/eqa/teaser.jpg" /></p>

<hr />

<h2 class="pubt">Evaluating Visual Conversational Agents via Cooperative Human-AI Games</h2>
<p class="pubd">
    <span class="authors">Prithvijit Chattopadhyay<sup>*</sup>, Deshraj Yadav<sup>*</sup>, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh</span><br />
    <span class="conf">HCOMP 2017</span><br />
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1708.05122">Paper</a>
        <a target="_blank" href="//github.com/VT-vision-lab/guesswhich">Code</a>
    </span>
</p>

<p><img src="/img/guesswhich/teaser.jpg" /></p>

<p><a name="/visdial-rl"></a></p>

<hr />

<h2 class="pubt">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Satwik Kottur<sup>*</sup>, Stefan Lee, José M.F. Moura, Dhruv Batra</span><br />
    <span class="conf">ICCV 2017 (Oral)</span><br />
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1703.06585">Paper</a>
        <a target="_blank" href="//github.com/batra-mlp-lab/visdial-rl">Code</a>
        <a target="_blank" href="//www.youtube.com/watch?v=R4hugGnNr7s">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=0B70NAN5i4ZHSaVBESEFHQW9vUk0">Slides</a>
    </span>
</p>

<p><img src="/img/visdial/qbot_abot.jpg" /></p>

<hr />

<h2 class="pubt">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h2>
<p class="pubd">
    <span class="authors">Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra</span><br />
    <span class="conf">IJCV 2019, ICCV 2017, NIPS 2016 Interpretable ML for Complex Systems Workshop</span><br />
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1610.02391">Paper</a>
        <a target="_blank" href="https://github.com/ramprs/grad-cam">Code</a>
        <a target="_blank" href="http://gradcam.cloudcv.org/">Demo</a>
    </span>
</p>

<p><img src="/img/grad-cam/teaser.png" /></p>

<p><a name="/visdial"></a></p>

<hr />

<h2 class="pubt">Visual Dialog</h2>
<p class="pubd" style="margin-bottom:20px;">
    <span class="authors">Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh, Dhruv Batra</span><br />
    <span class="conf">PAMI 2018, CVPR 2017 (Spotlight)</span><br />
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1611.08669">Paper</a>
        <a target="_blank" href="//github.com/batra-mlp-lab/visdial">Code</a>
        <a target="_blank" href="http://visualdialog.org/">visualdialog.org</a>
        <a target="_blank" href="https://github.com/batra-mlp-lab/visdial-amt-chat">AMT chat interface</a>
        <a target="_blank" href="http://demo.visualdialog.org">Demo</a>
        <a target="_blank" href="//www.youtube.com/watch?v=I9OlorMh7wU">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=0B70NAN5i4ZHSTWhRTTlMdVVIcFU">Slides</a>
    </span>
</p>

<p><img src="/img/visdial/teaser.png" /></p>

<!-- <div id="vimeo-embed">
    <iframe src="https://player.vimeo.com/video/193092429?byline=0&portrait=0&color=ffffff" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div> -->

<hr />

<h2 class="pubt">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</h2>

<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Harsh Agrawal<sup>*</sup>, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra</span> <br />
    <span class="conf">CVIU 2017, EMNLP 2016, ICML 2016 Workshop on Visualization for Deep Learning</span><br />
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1606.03556">Paper</a>
        <a target="_blank" href="https://abhishekdas.com/vqa-hat/">Project+Dataset</a>
        <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a>
    </span>
    <!-- Press: -->
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0; line-height: 1.1em;">
                <img src="/img/logos/nautilus.png" style="background: white; width: 57px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Is Artificial Intelligence Permanently Inscrutable?" by Aaron Bornstein</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/theverge.png" style="margin-right: 5px; background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Deep learning is creating computer systems we don't fully understand" by James Vincent</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/newscientist.jpg" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robot eyes and humans fix on different things to decode a scene" by Aviva Rutkin</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="http://www.techradar.com/news/world-of-tech/robots-and-humans-see-the-world-differently-but-we-don-t-know-why-1324165">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/techradar.png" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robots and humans see the world differently – but we don't know why" by Duncan Geere</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;" />
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"AI Is Learning to See the World—But Not the Way Humans Do" by Jamie Condliffe</span>
            </div>
        </a>
    </div>
</p>
<p><img src="/img/vqa-hat/teaser.jpg" /></p>
<hr />

<p><a name="/talks"></a></p>

<h1 id="talks">Talks</h1>

<div class="row">
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/visdial_rl_iccv17.jpg" />
        </p>
    </div>
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/embodiedqa_cvpr18_4.jpg" />
        </p>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38917625/tarmac-targeted-multiagent-communication">
                ICML 2019 Imitation, Intent, and Interaction Workshop:
                Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.facebook.com/icml.imls/videos/444326646299556/">
                ICML 2019 Oral: Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc">
                Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act"
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&amp;t=7330">
                CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s">
                CVPR 2018 Oral: Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">
                NVIDIA GTC 2018
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s">
                ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s">
                Visual Question Answering Challenge Workshop, CVPR 2017
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU">
                CVPR 2017 Spotlight: Visual Dialog
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/">
                Visualization for Deep Learning Workshop, ICML 2016
            </a>
        </div>
        <!-- <p class="talkd"> -->
            <!-- <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/"><img src="/img/talks/vqahat_icml16_deepviz.png"></a> -->
        <!-- </p> -->
    </div>
</div>
<hr />

<p><a name="/projects"></a></p>

<h1 id="side-projects">Side projects</h1>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aideadlin.es">aideadlin.es</a></h2>
        <p class="talkd">
            aideadlin.es is a webpage to keep track of CV/NLP/ML/AI conference deadlines. It's hosted on GitHub, and countdowns are automatically updated via pull requests to the data file in the repo.
            <a target="_blank" href="http://aideadlin.es"><img style="margin-top: 10px;" src="/img/projects/ai-deadlines-1547012831.png" /></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a></h2>
        <p class="talkd">
            Torch implementation of an attention-based visual question answering model (Yang et al., CVPR16).
            The model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.
            Some results <a href="https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/">here</a>.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg" /></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa">neural-vqa</a></h2>
        <p class="talkd">
            neural-vqa is an efficient, GPU-based Torch implementation of the visual question answering model from the NIPS 2015 paper 'Exploring Models and Data for Image Question Answering' by Ren et al.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg" /></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2>
        <p class="talkd">
            Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier.
            <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg" /></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2>
        <p class="talkd">
            graf plots pretty git contribution bar graphs in the terminal.
            <code>gem install graf</code> to install.
            <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif" /></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2>
        <p class="talkd">
            Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-)
            <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png" /></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2>
        <p class="talkd">
            AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>.
            <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg" /></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2>
        <p class="talkd">
            Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts).
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2>
        <p class="talkd">
            Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016).
        </p>
    </div>
</div>

<script src="/js/jquery.min.js"></script>

<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

<hr />


    </section>
</article>

            </div>
        </div>


        <!-- Footer
            ================================================== -->

        <!-- Javascripts
            ================================================= -->

        <!-- Analytics
        ================================================== -->
        <script>
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-42127951-1']);
            _gaq.push(['_trackPageview']);

            (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();

        </script>
    </body>
</html>
